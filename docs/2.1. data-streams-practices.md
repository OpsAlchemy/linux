
---

# Linux Streams Practice Lab: `STDIN`, `STDOUT`, `STDERR`, and Advanced Streams

This lab walks you through 15 exercises that build a strong foundation in Linux data streams. You’ll learn how to use standard input/output/error, redirection, pipes, real-time monitoring, and inter-process communication.

---

## Part 1: Core Stream Operations

---

### **Challenge 1: Redirect Output to a File**

**Task**  
Write the string `Learning Linux Streams` into a file named `streams.txt`.

**Solution**
```bash
echo "Learning Linux Streams" > streams.txt
```

**Concept**  
The `>` operator redirects standard output (STDOUT) to a file. If the file exists, it’s overwritten.

---

### **Challenge 2: Redirect Only Errors**

**Task**  
List contents of `/etc` and a non-existent directory. Redirect only the error output to a file named `errors.log`.

**Solution**
```bash
ls /etc /nonexistent 2> errors.log
```

**Concept**  
File descriptor `2` refers to standard error (STDERR). Using `2>` allows redirecting error messages separately from standard output.

---

### **Challenge 3: Combine Output and Errors**

**Task**  
Repeat the previous command, but redirect both STDOUT and STDERR to a file named `combined.log`.

**Solution**
```bash
ls /etc /nonexistent > combined.log 2>&1
```

**Concept**  
`2>&1` tells the shell to send STDERR to the same place as STDOUT. This way, all output (success + error) goes into one file.

---

### **Challenge 4: Read Input from User**

**Task**  
Create a Bash script that asks for user input and prints it back.

**Solution (`input_echo.sh`)**
```bash
#!/bin/bash
read -p "Enter something: " input
echo "You entered: $input"
```

**Concept**  
`read` takes input from STDIN (usually the keyboard). `echo` writes it to STDOUT.

---

### **Challenge 5: Read Input from a File**

**Task**  
Feed input into a script from a file instead of typing manually.

**Solution**
Create `input.txt`:
```bash
echo "streamed text" > input.txt
```

Run script:
```bash
./input_echo.sh < input.txt
```

**Concept**  
`<` redirects a file into the STDIN of a command or script.

---

## Part 2: Intermediate Stream Manipulation

---

### **Challenge 6: Filter Output via Pipes**

**Task**  
Create a file with multiple lines. Use `cat` and `grep` to display only lines containing a certain word.

**Solution**
```bash
cat > sample.txt <<EOF
apple
banana
grape
apple pie
EOF

cat sample.txt | grep apple
```

**Concept**  
The `|` (pipe) takes the STDOUT of `cat` and passes it as STDIN to `grep`. This is called stream chaining.

---

### **Challenge 7: Save and Display Output Simultaneously**

**Task**  
Display a message on the screen and save it to a file at the same time.

**Solution**
```bash
echo "Logging this" | tee output.log
```

**Concept**  
`tee` writes STDOUT to both the terminal and the specified file.

---

### **Challenge 8: Monitor File Changes in Real Time**

**Task**  
Follow changes to a log file live and display lines containing a specific keyword.

**Solution**
```bash
tail -f /var/log/syslog | grep "sshd"
```

**Concept**  
`tail -f` outputs new lines as they’re written to the file. Piping it to `grep` allows real-time filtering.

---

### **Challenge 9: Compare Directory Listings Using Process Substitution**

**Task**  
Compare contents of two directories using `diff` without temporary files.

**Solution**
```bash
diff <(ls /etc) <(ls /usr)
```

**Concept**  
`<(...)` is process substitution, which allows a command’s output to be treated like a file.

---

### **Challenge 10: Log CPU Usage with Timestamp**

**Task**  
Create a loop that logs CPU usage every second along with a timestamp.

**Solution**
```bash
while true; do
  echo "$(date +%T) $(vmstat 1 2 | tail -1 | awk '{print $13+$14}')"
  sleep 1
done | tee cpu_usage.log
```

**Concept**  
`vmstat` shows CPU stats, `date` provides timestamps, and `tee` logs to file while showing real-time output.

---

## Part 3: Advanced Streaming and IPC

---

### **Challenge 11: Use a Named Pipe (`mkfifo`)**

**Task**  
Create a FIFO, write to it from one terminal, and read from it in another.

**Solution**
```bash
mkfifo mypipe
```

Terminal 1:
```bash
cat mypipe
```

Terminal 2:
```bash
echo "Hello via pipe" > mypipe
```

**Concept**  
Named pipes enable communication between processes using the filesystem, acting as a stream between them.

---

### **Challenge 12: Stream Logs from a Remote Host**

**Task**  
Use SSH to stream logs from a remote machine and filter them.

**Solution**
```bash
ssh user@remote-host 'tail -f /var/log/syslog' | grep "nginx"
```

**Concept**  
Combines remote execution and local filtering. Useful for centralized log processing and monitoring.

---

### **Challenge 13: Real-Time Alert on Failed SSH Attempts**

**Task**  
Watch authentication logs and alert when failed login attempts occur.

**Solution**
```bash
tail -F /var/log/auth.log | grep --line-buffered "Failed password" | while read line; do
  echo "[ALERT] $line"
done
```

**Concept**  
Monitors a log file in real time, filters it, and triggers an action for each matching line.

---

### **Challenge 14: Format and Color Output from Stream**

**Task**  
Create a script that reads from STDIN and formats the output (e.g., with timestamps or color codes).

**Solution**
```bash
while read line; do
  echo -e "\033[1;32m[$(date +%T)] $line\033[0m"
done
```

Use it with:
```bash
cat sample.txt | ./formatter.sh
```

**Concept**  
Adding ANSI codes to STDOUT allows dynamic formatting in the terminal (e.g., colors, timestamps).

---

### **Challenge 15: Parallel Stream Processing**

**Task**  
Process a single stream in multiple ways: log it, compress it, and display it live.

**Solution**
```bash
your_command | tee >(gzip > out.gz) >(logger -t mystream)
```

**Concept**  
Combines `tee` with process substitution to split a stream into multiple destinations without duplicating commands.

---

## Summary

This lab covers:

- Core redirection (`>`, `<`, `2>`, `2>&1`)
- Input/output chaining via pipes
- Real-time monitoring
- Process substitution
- Named pipes for IPC
- Stream branching with `tee`
- Integration with remote systems and logging tools

This is foundational knowledge for Linux administration, DevOps automation, log aggregation, and real-time data processing.